{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data\n",
    "#### we'll be using the iris dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_iris()\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let us have a look at the dataset for the general understanding of type of data we are working with\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = data\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.054000           3.758667   \n",
       "std             0.828066          0.433594           1.764420   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.198667  \n",
       "std            0.763161  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X)\n",
    "print(iris.feature_names)\n",
    "df.columns = iris.feature_names\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = iris.feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions required for implementation of the tree\n",
    "##### Note: \n",
    "Information Gain func. has been used twice with slight modification to the second one. This is because we are dealing with continuous values in our dataset, hence we need to know at what value of a selected feature should we split the classes. Hence, the first function is to give information gain so that we can decide the split point. And the second function is so that we can decide the feature to split upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate- Entropy\n",
    "\n",
    "def entropy(classes,counts):\n",
    "    #counting total no of classes in the node\n",
    "    tot = counts.sum()\n",
    "    p = np.zeros(len(classes))\n",
    "    for i in range(len(classes)):\n",
    "        #calculating the probablities of each category in the classes present in the node\n",
    "        p[i] = float(counts[i])/tot\n",
    "    \n",
    "    #calculating entropy by formula\n",
    "    ent = -1 * p * np.log2(p)\n",
    "    return ent.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to calculate- Information Gain (FOR FINDING THE SPLIT POINT)\n",
    "\n",
    "def split_info_gain(X,Y,sel_feat):\n",
    "    #calculating the old entropy\n",
    "    classes, counts = np.unique(Y, return_counts=True)\n",
    "    old_ent = entropy(classes, counts)\n",
    "    \n",
    "    #calculating the new entropy\n",
    "    f_num = features.index(sel_feat)\n",
    "    cl, co = np.unique(X[:,f_num], return_counts=True) #calculating the no of classes and their counts in the next split\n",
    "    tot = co.sum()\n",
    "    \n",
    "    new_ent = np.zeros(len(cl))\n",
    "    wt = np.zeros(len(cl)) #array to store respective weights for each node\n",
    "    \n",
    "    for i in range(len(cl)):\n",
    "        y_ = Y[(X[:,f_num] == cl[i])]\n",
    "        classes, counts = np.unique(y_, return_counts=True)\n",
    "        new_ent[i] = entropy(classes, counts)\n",
    "        #calculating weights of each nodes for weighted addition later\n",
    "        wt[i] = float(co[i])/tot\n",
    "        \n",
    "    #new entropy is the weighted sum of entropies in all nodes in the next level\n",
    "    new_ent =  wt * new_ent\n",
    "    new_ent = new_ent.sum()\n",
    "\n",
    "    return (old_ent - new_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return the right split point for a selected feature\n",
    "\n",
    "def split_point(X,Y,sel_feat):\n",
    "    f_num = features.index(sel_feat)\n",
    "    x_ = np.copy(X[:,f_num])\n",
    "    \n",
    "    #finding the mid-points of all data points in the selected feature column \n",
    "    mid = np.zeros(len(x_)-1)\n",
    "    for i in range(len(x_)-2):\n",
    "        mid[i] = float(x_[i] + x_[i+1])/2\n",
    "    \n",
    "    max_info_gain = -1\n",
    "    sp_pt = -1\n",
    "    x = np.copy(X)\n",
    "    y = np.copy(Y)\n",
    "    \n",
    "    for m in mid:\n",
    "        #converting continuous values to categorical by checking if they lie above or below a certain mid-point\n",
    "        x_[x_ < m] = m-1\n",
    "        x_[x_ >= m] = m\n",
    "        x[:,f_num] = x_\n",
    "        #finding out which mid point in the whole column gives maximum gain in information\n",
    "        i_g = split_info_gain(x,y,sel_feat)\n",
    "        if (i_g > max_info_gain):\n",
    "            max_info_gain = i_g\n",
    "            sp_pt = m #storing split point\n",
    "        #reinitializing dummy variables to original values for next iteration\n",
    "        x_ = np.copy(X[:,f_num])\n",
    "        x = np.copy(X)\n",
    "        y = np.copy(Y)\n",
    "        \n",
    "    return sp_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert continuous values in the dataset into categorical ones whenever required\n",
    "def cont_2_cat(X,Y,sel_feat):\n",
    "    f_num = features.index(sel_feat)\n",
    "    x = X[:,f_num]\n",
    "    split_pt = split_point(X,Y,sel_feat)\n",
    "    #unlike the split_point function above, here we are actually converting contiuous\n",
    "    #values in the selected column into categorical ones for implementation of the algorithm\n",
    "    x[x >= split_pt] = split_pt + 1\n",
    "    x[x < split_pt] = split_pt - 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate- Information Gain (FOR CALCULATING THE GAIN RATIO TO SELECT THE FEATURE TO SPLIT UPON)\n",
    "\n",
    "def info_gain(X,Y,sel_feat):\n",
    "    #calculating the old entropy\n",
    "    classes, counts = np.unique(Y, return_counts=True)\n",
    "    old_ent = entropy(classes, counts)\n",
    "    \n",
    "    #calculating the new entropy\n",
    "    f_num = features.index(sel_feat)\n",
    "    #this is the only part that has been changed from the previous helper info_gain function \n",
    "    #we make dummy variables and pass them to cont_2_cat as we \n",
    "    #don't want the selected feature column to change just yet\n",
    "    x = np.copy(X)\n",
    "    #y = np.copy(Y)\n",
    "    cont_2_cat(x,Y,sel_feat)\n",
    "    #deviation from the previous helper info_gain function stop here\n",
    "    #rest everything is same as before\n",
    "    cl, co = np.unique(x[:,f_num], return_counts=True)\n",
    "    tot = co.sum()\n",
    "    \n",
    "    new_ent = np.zeros(len(cl)) #array to store respective entropies for each node after split\n",
    "    wt = np.zeros(len(cl)) #array to store respective weights for each node after split\n",
    "    \n",
    "    for i in range(len(cl)):\n",
    "        y_ = Y[(x[:,f_num] == cl[i])]\n",
    "        classes, counts = np.unique(y_, return_counts=True)\n",
    "        new_ent[i] = entropy(classes, counts)\n",
    "        #calculating weights of each nodes for weighted addition later\n",
    "        wt[i] = float(co[i])/tot\n",
    "    new_ent =  wt * new_ent\n",
    "    new_ent = new_ent.sum()\n",
    "    \n",
    "    if(new_ent == 0):\n",
    "        return 0.0 #in case of no split, the entropy remains same, hence info gain is indeed zero\n",
    "    else:\n",
    "        return (old_ent - new_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate- Split Info\n",
    "\n",
    "def split_info(X,Y,sel_feat):\n",
    "    f_num = features.index(sel_feat)\n",
    "    #we make dummy variables and pass them to cont_2_cat as we \n",
    "    #don't want the selected feature column to change just yet\n",
    "    x = np.copy(X)\n",
    "    #y = np.copy(Y)\n",
    "    cont_2_cat(x,Y,sel_feat)\n",
    "    cl, co = np.unique(x[:,f_num], return_counts=True)\n",
    "    a = np.zeros(len(cl))\n",
    "    b = np.zeros(len(cl))\n",
    "    tot = co.sum()\n",
    "    #check the split info formula; we need to know all the splits\n",
    "    #so iterating over all the splits in Y according to the classes in X\n",
    "    for i in range(len(cl)):\n",
    "        y = Y[(x[:,f_num] == cl[i])]\n",
    "        a[i] = float(len(y))/tot\n",
    "        b[i] = np.log2(a[i])\n",
    "    \n",
    "    return (-1 * a * b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate- Gain Ratio    \n",
    "def gain_ratio(X,Y,sel_feat):\n",
    "    ig = info_gain(X,Y,sel_feat)\n",
    "    if(ig > 0): \n",
    "        #no info gain means no splitting, hence this condition has been put\n",
    "        #to stop gain ratio from becoming undefined\n",
    "        return float(ig)/split_info(X,Y,sel_feat)\n",
    "    elif(ig <= 0):\n",
    "        return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build decision tree\n",
    "def dec_tree(X,Y,feat,level):\n",
    "    #printing the needed information as required \n",
    "    print(\"Level =\", level)\n",
    "    level += 1\n",
    "    classes, counts = np.unique(Y, return_counts=True)\n",
    "    for c in range(len(classes)):\n",
    "        print(\"Count of \",classes[c],\" = \",counts[c])\n",
    "    print(\"Current Entropy is =\", entropy(classes, counts))\n",
    "    \n",
    "    #checking for pure node or if the feature list is exhausted\n",
    "    if(len(feat) == 0 or len(np.unique(Y)) == 1):\n",
    "        print(\"Reached leaf Node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #initializing the variables necessary for calculations of Gain Ratio, feature to split etc.\n",
    "    g_r = -1\n",
    "    max_gain = -1\n",
    "    sel_feat = \"\"\n",
    "    \n",
    "    #comparing gain ratio for all features and deciding which feature to split upon\n",
    "    for f in feat:\n",
    "        g_r = gain_ratio(X, Y, f)\n",
    "        if(g_r > max_gain):\n",
    "            max_gain = g_r\n",
    "            sel_feat = f\n",
    "    #if pruning is needed, set the flag variable below to 1\n",
    "    flag = 0\n",
    "    if(max_gain == 0 and flag == 1):\n",
    "        print(\"Reached leaf Node\")\n",
    "        print()\n",
    "        return\n",
    "        \n",
    "    print(\"Splitting on feature\",sel_feat, \"with gain ratio\", max_gain)\n",
    "    print()\n",
    "\n",
    "    f_num = features.index(sel_feat)\n",
    "    #when feature is finally selected, we actually convert the column into categorical data\n",
    "    #as now we'll split Y into different nodes based on the selected feature\n",
    "    cont_2_cat(X,Y,sel_feat)\n",
    "    \n",
    "    #feature has been selected, checking the number of categories in the selected feature\n",
    "    cl = np.unique(X[:,f_num])\n",
    "    feat.remove(sel_feat)\n",
    "    #splitting Y into different nodes according to categories found in selected feature\n",
    "    for i in cl:\n",
    "        x_tr = X[(X[:,f_num] == i)]\n",
    "        y_tr = Y[(X[:,f_num] == i)]\n",
    "        dec_tree(x_tr, y_tr, feat,level)\n",
    "        \n",
    "        \n",
    "    #adding the feature in features list again for previous recursive calls\n",
    "    feat.insert(f_num,sel_feat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level = 0\n",
      "Count of  0  =  50\n",
      "Count of  1  =  50\n",
      "Count of  2  =  50\n",
      "Current Entropy is = 1.584962500721156\n",
      "Splitting on feature petal width (cm) with gain ratio 0.9999999999999999\n",
      "\n",
      "Level = 1\n",
      "Count of  0  =  50\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level = 1\n",
      "Count of  1  =  50\n",
      "Count of  2  =  50\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature petal length (cm) with gain ratio 0.6621582046482519\n",
      "\n",
      "Level = 2\n",
      "Count of  1  =  44\n",
      "Count of  2  =  1\n",
      "Current Entropy is = 0.15374218032876188\n",
      "Splitting on feature sepal length (cm) with gain ratio 0.1886274738620596\n",
      "\n",
      "Level = 3\n",
      "Count of  1  =  3\n",
      "Count of  2  =  1\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature sepal width (cm) with gain ratio 0.15106563978903303\n",
      "\n",
      "Level = 4\n",
      "Count of  1  =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level = 4\n",
      "Count of  1  =  2\n",
      "Count of  2  =  1\n",
      "Current Entropy is = 0.9182958340544896\n",
      "Reached leaf Node\n",
      "\n",
      "Level = 3\n",
      "Count of  1  =  41\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level = 2\n",
      "Count of  1  =  6\n",
      "Count of  2  =  49\n",
      "Current Entropy is = 0.4971677614160753\n",
      "Splitting on feature sepal length (cm) with gain ratio 0.05463893158092842\n",
      "\n",
      "Level = 3\n",
      "Count of  1  =  6\n",
      "Count of  2  =  37\n",
      "Current Entropy is = 0.5830194167347008\n",
      "Splitting on feature sepal width (cm) with gain ratio 0.0519480903515746\n",
      "\n",
      "Level = 4\n",
      "Count of  1  =  6\n",
      "Count of  2  =  32\n",
      "Current Entropy is = 0.6292492238560345\n",
      "Reached leaf Node\n",
      "\n",
      "Level = 4\n",
      "Count of  2  =  5\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level = 3\n",
      "Count of  2  =  12\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = iris.feature_names\n",
    "feat=features.copy()\n",
    "\n",
    "#building decision tree\n",
    "dec_tree(X,Y,feat,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DT.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
